{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.0.8'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kodowanie słów i znaków metodą gorącej jedynki\n",
    "\n",
    "Kodowanie metodą gorącej jedynki jest najpopularniejszym podstawowym sposobem zamieniania tokena w wektor. Z metody tej korzystaliśmy w początkowych przykładach przetwarzania zbiorów IMDB i Agencji Reutera przedstawionych w rozdziale 3. (używaliśmy jej do kodowania słów). Polega ona na przypisaniu do każdego słowa unikatowego indeksu będącego wartością całkowitoliczbową i umieszczoną w wektorze binarnym o długości N (rozmiar słownika). Wektor przyjmuje same wartości zerowe poza i-tym elementem, który przyjmuje wartość 1.\n",
    "\n",
    "Oczywiście metoda ta może być również użyta na poziomie znaków. W celu wyjaśnienia praktycznej implementacji metody kodowania z gorącą jedynką chciałbym zaprezentować dwa przykłady: pierwszy z nich przedstawia kodowanie słów, a drugi kodowanie znaków.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prosty przykład kodowania słów metodą gorącej jedynki:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Początkowa forma danych: jeden element na próbkę \n",
    "# (w tym przykładzie próbką jest zdanie, ale może ona być również całym dokumentem).\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "\n",
    "# Zbuduj indeks wszystkich tokenów danych.\n",
    "token_index = {}\n",
    "for sample in samples:\n",
    "    # Tokenizacja próbek poprzez metodę podziału. \n",
    "    # Podczas pracy z prawdziwymi danymi podziału dokonuje się również na znakach interpunkcyjnych i specjalnych.\n",
    "    for word in sample.split():\n",
    "        if word not in token_index:\n",
    "            # Przypisywanie unikatowego indeksu do każdego unikatowego słowa.\n",
    "            token_index[word] = len(token_index) + 1\n",
    "            # Zwróć uwagę na to, że indeks 0 nie jest przypisywany do żadnego słowa.\n",
    "\n",
    "# Wektoryzacja próbek. Bierzemy pod uwagę tylko max_length pierwszych słów każdej próbki.\n",
    "max_length = 10\n",
    "\n",
    "# Tu przechowujemy wyniki operacji:\n",
    "results = np.zeros((len(samples), max_length, max(token_index.values()) + 1))\n",
    "for i, sample in enumerate(samples):\n",
    "    for j, word in list(enumerate(sample.split()))[:max_length]:\n",
    "        index = token_index.get(word)\n",
    "        results[i, j, index] = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prosty przykład kodowania znaków metodą gorącej jedynki:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "characters = string.printable  # Wszystkie znaki ASCII, które można wyświetlić.\n",
    "token_index = dict(zip(characters, range(1, len(characters) + 1)))\n",
    "\n",
    "max_length = 50\n",
    "results = np.zeros((len(samples), max_length, max(token_index.values()) + 1))\n",
    "for i, sample in enumerate(samples):\n",
    "    for j, character in enumerate(sample[:max_length]):\n",
    "        index = token_index.get(character)\n",
    "        results[i, j, index] = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pakiet Keras jest wyposażony w narzędzia przeznaczone do kodowania znaków i słów metodą gorącej jedynki (narzędzia te potrafią przetwarzać surowy tekst). W praktyce warto z nich korzystać, ponieważ wykonują wiele ważnych operacji, takich jak usuwanie znaków specjalnych i branie pod uwagę tylko N słów najczęściej występujących w zbiorze (zwykle stosuje się takie ograniczenie w celu uniknięcia pracy z bardzo dużymi przestrzeniami wektora wejściowego)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przykład kodowania słów metodą gorącej jedynki przy użyciu gotowych narzędzi pakietu Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "\n",
    "# Tworzy mechanizm tokenizacji skonfigurowany tak, \n",
    "# aby brał pod uwagę tylko 100 najczęściej występujących słów.\n",
    "tokenizer = Tokenizer(num_words=1000)\n",
    "# Buduje indeks słów.\n",
    "tokenizer.fit_on_texts(samples)\n",
    "\n",
    "# Zamienia łańcuchy na listy indeksów (wartości całkowitoliczbowe).\n",
    "sequences = tokenizer.texts_to_sequences(samples)\n",
    "\n",
    "# Możliwe jest również uzyskanie bezpośredniej binarnej reprezentacji kodowania metodą gorącej jedynki.\n",
    "# Ten generator tokenów obsługuje także inne tryby wektoryzacji.\n",
    "one_hot_results = tokenizer.texts_to_matrix(samples, mode='binary')\n",
    "\n",
    "# Przykład kodu pozwalającego na uzyskanie dostępu do indeksu słów.\n",
    "word_index = tokenizer.word_index\n",
    "print('Znaleziono %s unikatowych tokenów.' % len(word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Odmianą kodowania metodą gorącej jedynki, która może zostać użyta, gdy liczba unikatowych tokenów w słowniku jest zbyt duża, aby obsłużyć ją w sposób jawny, jest sztuczka haszowania z gorącą jedynką (ang. one-hot hashing trick). Zamiast jawnie przypisywać indeks do każdego ze słów i utrzymywać odwołania do tych indeksów w słowniku, można haszować słowa do formy wektorów o określonym rozmiarze. Zwykle robi się to za pomocą bardzo lekkiej funkcji haszującej. Główną zaletą tej metody jest brak konieczności utrzymywania jawnego indeksu słów, co pozwala oszczędzić przestrzeń pamięci i zakodować dane w locie (możliwe jest natychmiastowe wygenerowanie wektorów tokenu, bez potrzeby przyglądania się całości dostępnych danych). Wadą tego rozwiązania jest możliwość wystąpienia konfliktów haszy (ang. hash collisions), polegających na przepisaniu tego samego hasza dwóm różnym słowom (w takim przypadku żaden model uczenia maszynowego analizujący uzyskane hasze nie będzie mógł odróżnić od siebie tych słów). Prawdopodobieństwo wystąpienia konfliktów haszy maleje, gdy przestrzeń haszowania jest o wiele większa od całkowitej liczby unikatowych haszowanych tokenów."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prosty przykład sztuczki haszowania słów metodą gorącej jedynki:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "\n",
    "# Słowa są zapisywane w postaci wektorów o długości 1000. \n",
    "# Jeżeli przetworzymy przykład, w którym znajduje się około 1000 różnych słów,\n",
    "# to zauważymy wiele konfliktów haszy, \n",
    "# które doprowadzą do pogorszenia dokładności tej metody kodowania.\n",
    "dimensionality = 1000\n",
    "max_length = 10\n",
    "\n",
    "results = np.zeros((len(samples), max_length, dimensionality))\n",
    "for i, sample in enumerate(samples):\n",
    "    for j, word in list(enumerate(sample.split()))[:max_length]:\n",
    "        # Słowom przypisywane są losowe wartości całkowite indeksu z zakresu od 0 do 1000.        \n",
    "        index = abs(hash(word)) % dimensionality\n",
    "        results[i, j, index] = 1."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
